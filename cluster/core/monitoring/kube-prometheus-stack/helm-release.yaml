---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 30m
  timeout: 15m
  chart:
    spec:
      chart: kube-prometheus-stack
      interval: 5m
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
      version: 67.0.0
  install:
    remediation:
      retries: 3
  upgrade:
    force: true
    remediation:
      retries: 3
  values:
    crds:
      enabled: true
    cleanPrometheusOperatorObjectNames: true
    alertmanager:
      alertmanagerSpec:
        replicas: 1
        storage:
          volumeClaimTemplate:
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi
      config:
        global:
          resolve_timeout: 5m
          slack_api_url: ${SECRET_ALERT_MANAGER_DISCORD_WEBHOOK}
        inhibit_rules:
          - equal:
              - alertname
              - namespace
            source_match:
              severity: critical
            target_match:
              severity: warning
        receivers:
          - name: "null"
          - name: opsgenie
            opsgenie_configs:
              - api_key: ${SECRET_OPSGENIE_API_KEY}
                description: |-
                  {{ if gt (len .Alerts.Firing) 0 -}}
                  Alerts Firing:
                  {{ range .Alerts.Firing }}
                  - Message: {{ .Annotations.message }}
                    Labels:
                  {{ range .Labels.SortedPairs }}   - {{ .Name }} = {{ .Value }}
                  {{ end }}   Annotations:
                  {{ range .Annotations.SortedPairs }}   - {{ .Name }} = {{ .Value }}
                  {{ end }}   Source: {{ .GeneratorURL }}
                  {{ end }}
                  {{- end }}
                  {{ if gt (len .Alerts.Resolved) 0 -}}
                  Alerts Resolved:
                  {{ range .Alerts.Resolved }}
                  - Message: {{ .Annotations.message }}
                    Labels:
                  {{ range .Labels.SortedPairs }}   - {{ .Name }} = {{ .Value }}
                  {{ end }}   Annotations:
                  {{ range .Annotations.SortedPairs }}   - {{ .Name }} = {{ .Value }}
                  {{ end }}   Source: {{ .GeneratorURL }}
                  {{ end }}
                  {{- end }}
                  View Alert in Karma:
                    https://karma.${SECRET_DOMAIN}/?q=%40receiver%3D{{ .Receiver | urlquery }}&q=%40state%3Dactive
                details:
                  alertname: "{{ .GroupLabels.alertname }}"
                  deployment: "{{- if .CommonLabels.deployment -}}{{- .CommonLabels.deployment -}}{{- end -}}"
                  namespace: "{{- if .CommonLabels.exported_namespace -}}{{- .CommonLabels.exported_namespace -}}{{- else if .CommonLabels.namespace -}}{{- .CommonLabels.namespace -}}{{- end -}}"
                  pod: "{{- if .CommonLabels.pod -}}{{- .CommonLabels.pod -}}{{- end -}}"
                  severity: "{{ .GroupLabels.severity }}"
                message: |-
                  [{{ .Status | toUpper -}}
                  {{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{- end -}}
                  ] {{ .CommonLabels.alertname }}
                priority: '{{ if eq .GroupLabels.severity "critical" }}P1{{ else if eq .GroupLabels.severity "warning" }}P2{{ else if eq .GroupLabels.severity "info" }}P3{{ else }}P4{{ end }}'
                responders:
                  - id: ab411aaa-0f56-477a-9028-13cd801e5213
                    type: team
                send_resolved: true
                tags: "{{ .GroupLabels.severity }}, {{ .GroupLabels.alertname }}, {{ .GroupLabels.namespace }}, {{- if .CommonLabels.exported_namespace -}}{{ .CommonLabels.exported_namespace }},{{- end -}}"
          - name: discord
            slack_configs:
              - channel: "#k8s-uk-home-notifications"
                icon_url: https://avatars3.githubusercontent.com/u/3380462?s=200&v=4
                send_resolved: true
                text: |-
                  {{ range .Alerts -}}
                    **Alert:** {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}

                  **Description:** {{ if ne .Annotations.description ""}}{{ .Annotations.description }}{{else}}N/A{{ end }}
                  **Details:**
                    {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
                    {{ end }}
                  {{ end }}
                title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if ne .CommonAnnotations.summary ""}}{{ .CommonAnnotations.summary }}{{ else }}{{ .CommonLabels.alertname }}{{ end }}'
                username: Prometheus
        route:
          group_by:
            - alertname
            - namespace
            - severity
          group_interval: 5m
          group_wait: 30s
          receiver: opsgenie
          repeat_interval: 6h
          routes:
            - match:
                alertname: Watchdog
              receiver: "null"
            - continue: true
              receiver: opsgenie
      ingress:
        enabled: false
    fullnameOverride: prometheus
    grafana:
      enabled: false
      forceDeployDashboards: true
      sidecar:
        dashboards:
          multicluster:
            etcd:
              enabled: false
    kube-state-metrics:
      fullnameOverride: kube-state-metrics
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels:
                - __meta_kubernetes_pod_node_name
              targetLabel: kubernetes_node
    kubeApiServer:
      enabled: true
    kubeControllerManager:
      enabled: true
    kubeEtcd:
      enabled: false
      endpoints:
        - ${SECRET_K3S_CONTROL_PLANE_ADDRESS_1}
      service:
        enabled: true
        port: 2381
        targetPort: 2381
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: true
      endpoints:
        - ${SECRET_K3S_CONTROL_PLANE_ADDRESS_1}
      service:
        port: 10250
        targetPort: 10250
    kubelet:
      enabled: true
      serviceMonitor:
        metricRelabelings:
          - action: replace
            sourceLabels:
              - node
            targetLabel: instance
    nameOverride: ""
    prometheus:
      ingress:
        enabled: false
      prometheusSpec:
        scrapeInterval: 1m # Must match interval in Grafana Helm chart
        containers:
          - name: prometheus
            startupProbe:
              failureThreshold: 120 # periodSeconds is 15 so this would give (120 x 15s = 1800s) for the WAL replay
              initialDelaySeconds: 600 # 10 minutes before even checking the probe
        additionalScrapeConfigs:
          - honor_timestamps: true
            job_name: minio
            metrics_path: /minio/v2/metrics/cluster
            bearer_token: ${SECRET_MINIO_MONITORING_BEARER_TOKEN}
            static_configs:
              - targets:
                  - minio.tools:9000
          - job_name: felix_metrics
            kubernetes_sd_configs:
              - role: endpoints
            relabel_configs:
              - action: keep
                regex: felix-metrics-svc
                replacement: $1
                source_labels:
                  - __meta_kubernetes_service_name
            scheme: http
            scrape_interval: 5s
          - job_name: kube_controllers_metrics
            kubernetes_sd_configs:
              - role: endpoints
            relabel_configs:
              - action: keep
                regex: kube-controllers-metrics-svc
                replacement: $1
                source_labels:
                  - __meta_kubernetes_service_name
            scheme: http
            scrape_interval: 5s
        enableAdminAPI: true
        nodeSelector:
          kubernetes.io/arch: amd64
          node_locality: internal
        podAntiAffinity: hard
        podMonitorNamespaceSelector: {}
        podMonitorSelector: {}
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        replicaExternalLabelName: replica
        replicas: 1
        retention: 30d
        retentionSize: 75GB
        ruleSelectorNilUsesHelmValues: false
        scrapeConfigSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        enableFeatures:
          - auto-gomemlimit
          - memory-snapshot-on-shutdown
          - new-service-discovery-manager
        resources:
          requests:
            cpu: 50m
            memory: 350Mi
          limits:
            memory: 2500Mi
        storageSpec:
          volumeClaimTemplate:
            spec:
              resources:
                requests:
                  storage: 50Gi
        walCompression: true
    prometheus-node-exporter:
      fullnameOverride: node-exporter
      extraArgs:
        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
        - --collector.systemd
        - --collector.processes
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels:
                - __meta_kubernetes_pod_node_name
              targetLabel: kubernetes_node
    prometheusOperator:
      fullnameOverride: prometheus-operator
      admissionWebhooks:
        certManager:
          enabled: true
    additionalPrometheusRulesMap:
      dockerhub-rules:
        groups:
          - name: dockerhub
            rules:
              - alert: DockerhubRateLimitRisk
                annotations:
                  summary: Kubernetes cluster Dockerhub rate limit risk
                expr: count(time() - container_last_seen{image=~"(docker.io).*",container!=""} < 30) > 100
                labels:
                  severity: critical
      oom-rules:
        groups:
          - name: oom
            rules:
              - alert: OomKilled
                annotations:
                  summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
                expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
                labels:
                  severity: critical
